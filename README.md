

### 2026 LLM Evaluation ë…¼ë¬¸ ìŠ¤í„°ë””

| Index | ì£¼ì œ | ë…¼ë¬¸ | ë°œí‘œì | ë°œí‘œìë£Œ | ì˜ìƒ |
| :---- | :---- | :---- | :---- | :---- | :---- |
| 1 | LLM í‰ê°€ ê°œê´„ | A Survey on Evaluation of Large Language Models [\[ë§í¬\]](https://arxiv.org/abs/2307.03109)<br>A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations [\[ë§í¬\]](https://arxiv.org/abs/2407.04069) | ê¹€ê¸°ë²” | [ğŸ“„](data/01%20LLM%20Evaluation_%20Overview.pdf) | [ğŸ¥](https://www.youtube.com/watch?v=RMOZFEo9Yzo) |
| 2 | Long-Context | Needle in a Haystack<br>LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks<br>InfiniteBench: Extending Long Context Evaluation Beyond 100K Tokens<br>RULER: Whatâ€™s the Real Context Size of Your Long-Context Language Models?<br>A Controllable Examination for Long-Context Language Models | ì¡°ë™í—Œ | [ğŸ“„](data/02%20Long%20Context%20Benchmark.pdf) | [ğŸ¥](https://www.youtube.com/watch?v=ykrqYxultSs) |
| 2-1 | Long-Context(Satellite) | LongBench pro [\[ë§í¬\]](https://arxiv.org/abs/2601.02872) | ê¹€ê¸°ë²” | [ğŸ“„](data/02-1%20LongBench%20Pro.pdf) | [ğŸ¥](https://www.youtube.com/watch?v=CRL8h_cLjh8) |
| 3 | ì§€í‘œ ë¶•ê´´, Goodhart's law | The Leaderboard Illusion [\[ë§í¬\]](https://arxiv.org/abs/2504.20879)<br>Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models [\[ë§í¬\]](https://arxiv.org/abs/2502.14318) | í•œì™„ê·œ | [ğŸ“„](data/03%20llm_evaluation_illusion.pdf) | [ğŸ¥](https://www.youtube.com/watch?v=Oy390XforzM) |
| 4 | Software Engineering | SWE-bench: Can Language Models Resolve Real-World GitHub Issues? [\[ë§í¬\]](https://arxiv.org/abs/2310.06770), Multimodal version [\[ë§í¬\]](https://arxiv.org/abs/2410.03859)<br>LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code[\[ë§í¬\]](https://arxiv.org/abs/2403.07974) | ë°•ì§„ìš° | | |
| 5 | Agents - Tool Use | AGENTBENCH: Evaluating LLMs as Agents[\[ë§í¬\]](https://arxiv.org/abs/2308.03688) <br>StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models[\[ë§í¬\]](https://arxiv.org/abs/2403.07714) <br>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers[\[ë§í¬\]](https://openreview.net/forum?id=ffYd6uJpJE) | ê¹€ê°•ë¯¼ | | |
| 6 | Agents - End to End | GAIA: A Benchmark for General AI Assistants [\[ë§í¬\]](https://arxiv.org/abs/2311.12983)<br>WebArena: A Realistic Web Environment for Building Autonomous Agents [\[ë§í¬\]](https://arxiv.org/abs/2307.13854)<br>An Illusion of Progress? Assessing the Current State of Web Agents(Online-Mind2Web) [\[ë§í¬\]](https://arxiv.org/abs/2504.01382)<br>MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering[\[ë§í¬\]](https://openreview.net/forum?id=6s5uXNWGIh) | ê¹€ë™í˜„ | | |
| 7 | Knowledge | GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks [\[ë§í¬\]](https://arxiv.org/abs/2510.04374)<br>GPQA: A Graduate-Level Google-Proof Q&A Benchmark [\[ë§í¬\]](https://arxiv.org/abs/2311.12022) | ê¹€ì •í›ˆ | | |
| 8 | Textual & Content Safety | HarmBench: A Standardized Evaluation Framework for Automated Red Teaming[\[ë§í¬\]](https://arxiv.org/abs/2402.04249) <br>A StrongREJECT for Empty Jailbreaks[\[ë§í¬\]](https://arxiv.org/abs/2402.10260)<br>TrustLLM: Trustworthiness in LLMs [\[ë§í¬\]](https://arxiv.org/abs/2401.05561)| í™ì†Œí˜„ | | |
| 9 | Agentic & Behavioral Safety | Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents [\[ë§í¬\]](https://arxiv.org/abs/2410.02644)<br>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents[\[ë§í¬\]](https://arxiv.org/abs/2410.09024)| ì´ë™ê±´ | | |
| 10 | LLM-as-a-Judge | Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models [\[ë§í¬\]](https://arxiv.org/abs/2405.01535)<br>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena [\[ë§í¬\]](https://openreview.net/forum?id=uccHPGDlao)<br>JudgeBench: A Benchmark for Evaluating Judges [\[ë§í¬\]](https://arxiv.org/abs/2410.12784) | ì¡°ì„±êµ­ | | |
| 11 | Multimodal Reasoning | MMMU Benchmark [\[ë§í¬\]](https://arxiv.org/abs/2311.16502)<br>Humanity's Last Exam [\[ë§í¬\]](https://arxiv.org/abs/2501.14249) | ìµœë™í˜ | | |
| 12 | Thinking Process & Reasoning | Measuring Faithfulness in Chain-of-Thought Reasoning[\[ë§í¬\]](https://arxiv.org/abs/2307.13702)<br>Evaluating Mathematical Reasoning Beyond Accuracy[\[ë§í¬\]](https://arxiv.org/abs/2404.05692) | ë°•ì§„í˜• | | |
| 13 | Evaluation in Investment & Quant | TBD | ê¹€í˜„ì¬ | | |

