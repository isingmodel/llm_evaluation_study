

### 2026 LLM Evaluation 논문 스터디


| 주차 | 주제 | 논문 | 발표자 |
| :---- | :---- | :---- | :---- |
| 1주 | LLM 평가 개괄 | A Survey on Evaluation of Large Language Models [\[링크\]](https://arxiv.org/abs/2402.06196) | 김기범 |
| 2주 | Knowledge | GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks [\[링크\]](https://arxiv.org/abs/2510.04374)<br>GPQA: A Graduate-Level Google-Proof Q\&A Benchmark [\[링크\]](https://arxiv.org/abs/2311.12022) |  |
| 3주 | Software Engineering | SWE-bench: Can Language Models Resolve Real-World GitHub Issues? [\[링크\]](https://arxiv.org/abs/2310.06770), Multimodal version [\[링크\]](https://arxiv.org/abs/2410.03859)<br>LiveBench: A Challenging, Contamination-Limited LLM Benchmark [\[링크\]](https://arxiv.org/abs/2406.19314) |  |
| 4주 | Long-Context | LongBench v2 [\[링크\]](https://arxiv.org/abs/2412.15204)<br>Multimodal Needle in a Haystack [\[링크\]](https://arxiv.org/abs/2406.11230) |  |
| 5주 | Agents | GAIA: A Benchmark for General AI Assistants [\[링크\]](https://arxiv.org/abs/2311.12983) WebArena: A Realistic Web Environment for Building Autonomous Agents [\[링크\]](https://webarena.dev/)<br>An Illusion of Progress? Assessing the Current State of Web Agents(Online-Mind2Web) [\[링크\]](https://arxiv.org/abs/2504.01382) |  |
| 6주 | Safety & Red Teaming | Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents [\[링크\]](https://arxiv.org/abs/2410.02644) TrustLLM: Trustworthiness in LLMs [\[링크\]](https://trustllmbenchmark.github.io/TrustLLM-Website/) |  |
| 7주 | 지표 붕괴, Goodhart’s law | The Leaderboard Illusion [\[링크\]](https://arxiv.org/abs/2504.20879)<br>Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models [\[링크\]](https://arxiv.org/abs/2502.14318) |  |
| 8주 | LLM-as-a-Judge | Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models [\[링크\]](https://arxiv.org/abs/2405.01535)<br>Judging LLM-as-a-Judge with MT-Bench nd Chatbot Arena [\[링크\]](https://arxiv.org/pdf/2306.05685) JudgeBench: A Benchmark for Evaluating Judges [\[링크\]](https://arxiv.org/abs/2410.12784) |  |
| 9주 | Multimodal Reasoning | MMMU Benchmark [\[링크\]](https://mmmu-benchmark.github.io/)<br>Humanity's Last Exam [\[링크\]](https://arxiv.org/abs/2501.14249) |  |
| 10주 | Thinking Process & Reasoning | Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge [\[링크\]](https://arxiv.org/abs/2501.18099)<br>ARC-AGI-2 [\[링크\]](https://arxiv.org/abs/2505.11831) |  |

